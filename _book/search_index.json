[
["index.html", "Osburn Lab Protocols 1 | About", " Osburn Lab Protocols By Caitlin Casar Created: 2019-10-16 Last updated: 2019-11-19 1 | About This is a collection of protocols for the Osburn Lab üòé Figure 1.1: Osburn Lab Spring 2019 "],
["data-access.html", " 2 | Data Access + Storage 2.1 MacOS Users 2.2 Windows Users", " 2 | Data Access + Storage Created: 2019-10-16 The Osburn Lab data is backed up to RDSS at Northwestern. You can access this data if you have permissions using the directions here. If you do not currently have access, you will need to be added as a user by Caitlin or Maggie. 2.1 MacOS Users Open Finder and navigate to Go &gt; Connect to Server‚Ä¶ Add this server address: smb://resfiles.northwestern.edu/OSBURN_LAB Add your netID and password. Finder will automatically open OSBURNLAB in your system Volumes. To navigate to the OSBURNLAB dirctory in your terminal: cd /Volumes/OSBURN_LAB 2.2 Windows Users Open windows file explore and add this server address: resfiles.northwestern.edu You will be prompted for your user name (netID) and password. The contents of OSBURNLAB is displayed in the window. "],
["version-control.html", " 3 | Version Control", " 3 | Version Control Created: 2019-10-16 If you‚Äôre writing code, it‚Äôs very important to implement version control with Git. This guide will get you started! First you‚Äôll need to install Git. Next, create an account on Github. If you want access to provate repositories (i.e. if you need to backup unpublished data or code), be sure to set up a student account. Now, you‚Äôll need to set your credentials in Git. Open up your terminal. #set your user name on github git config --global user.name &quot;John Doe&quot; #set your user email on github git config --global user.email johndoe@example.com Now, go to Github and create a repository for your code. If you want this repository to be private, change the repo settings on Githib. Click on the settings button. Then set the reposotory to private. Then clone this repository to your computer. You may be prompted to enter your Github password. #change directories to the desired location for your repository cd ~/Desktop #clone your repository using the URL git clone https://github.com/OsburnLab/Protocols Now you can add your code files to this cloned repository. When you‚Äôre done editing your code, push it up to the Github server. #add your new files to the queue git add . #commit your changes and add a short description git commit -a -m &quot;short description here&quot; #push your changes to the Github server git push "],
["create-a-protocol.html", " 4 | Create a Protocol", " 4 | Create a Protocol Created: 2019-10-16 Wanna share a cool protocol with your lab mates in this bookdown document? Follow this guide to learn how! First, you‚Äôll need to clone this repository using Git in your terminal. #change directories to a desirable location cd ~/Desktop git clone https://github.com/OsburnLab/Protocols Next, open the bookdown-demo.Rproj file in RStudio. Then, select File &gt; New File &gt; R Markdown‚Ä¶ Give this file a name in the format ‚Äònumber-name.Rmd‚Äô, where number is in sequential order with the other .Rmd files. Add an H1 element chapter title to the file. # | Chapter Title Next, add some paragraph content below this. # | Chapter Title Here is some paragraph content. To add a code chunk, equations, or figures, check out this link. When you‚Äôre done editing the R markdown file, render the book. bookdown::render_book(&quot;index.rmd&quot;, &quot;bookdown::gitbook&quot;) Now it‚Äôs time to update your changes online! #change directories to the protocols folder cd ~/Desktop/Protocols #add all new files you created git add. #commit all of your changes and add a short description about your update git commit -a -m &quot;short description here&quot; #push your changes to the github server git push #update the rendered html file on the github-hosted page cd _book git add . git commit -a -m &quot;short description here&quot; git push "],
["qiime2-workflow.html", " 5 | Qiime2 workflow 5.1 Import data 5.2 Demultiplexing 5.3 Denoising 5.4 Clustering 5.5 Taxonomy 5.6 Taxa barplots and diversity analyses in Qiime2", " 5 | Qiime2 workflow Created: 2019-11-19 Created by: Matt Selensky Workflow for 16S amplicon sequence analysis in Qiime2 5.1 Import data This protocol is designed for the processing of 16S rRNA gene amplicon data using 515F/806R primers. Qiime2 requires us to convert the raw data the sequencing center sends us into Qiime-zipped artifacts, or a .qza extension. We must first import our data (paired-end .fastq files) into this format: qiime tools import --type EMPPairedEndSequences --input-path emp-paired-end-sequences --output-path emp-paired-end-sequences.qza The above function requires three .fastq files. One for the forward reads, one for the reverse reads, and another for the barcodes. They must be named forward.fastq.gz, reverse.fastq.gz, and barcodes.fastq.gz, respectively. qiime tools import will yield a single output, emp-paired-end-sequences.qza, that will contain all of the barcoded reads from every single sample submitted to the sequencing center. 5.2 Demultiplexing At the sequencing center, DNA sequences were given a barcode specific to each sample so we can track which sample our reads in the emp-paired-end-sequences.qza file originated from. We do this by demultiplexing our sequences. In Qiime2, we need to create a metadata file that contains the barcodes used for each sample. Check out this example from the Qiime2 documentation of how this metadata file should be formatted. The sequencing center should send a mapping file from which you can obtain the barcodes for each sample. Be sure to save the metadata file as a .tsv. Specify the barcodes and other associated metadata only for the samples you are interested in analyzing. As is often the case in our lab, sequencing data is typically sent back as a mix of samples from different projects. Only including your samples in the metadata file will subset the large .qza file and will significantly cut down on computation time. Because we are demultiplexing EMP paired-end sequences, we should use the demux emp-paired command. The column which contains the barcode in the metadata file for each sample must be specified using the argument barcodes-column: qiime demux emp-paired --m-barcodes-file sample-metadata.tsv --m-barcodes-column barcode-sequence --i-seqs emp-paired-end-sequences.qza --o-per-sample-sequences demux.qza Note: if you have reverse complement sequences, you must pass the argument, --p-rev-comp-mapping-barcodes to your demux command to account for this. You can look at this on the Qiime2 viewer by producing a Qiime-zipped visualization file, .qzv, from your now-demultiplexed .qza output: qiime demux summarize --i-data demux.qza --o-visualization demux.qzv From the interactive quality plot in demux.qzv, we can see the distribution of quality scores for each sequenced base. If analyzing paired-end data, you will see two plots: one for the forward read, and one for the reverse read. We use this visualization to inform how we will trim and truncate the ends of the reads in the next denoising step using dada2 denoise-paired 5.3 Denoising We will use the DADA2 algorithm to denoise our data. DADA2 is a robust way to filter out noisy sequences, correct errors in marginal sequences, remove chimeras, remove singletons, join denoised paired-end reads, and dereplicate sequences. Previously, each of these functions would require separate commands, but DADA2 does it all-in-one. Therefore, this is a particularly computationally intense process. One should consider running dada2 on a computer that can handle it (perhaps by accessing Northwestern‚Äôs high-performance computing cluster, Quest). qiime dada2 denoise-paired --i-demultiplexed-seqs demux.qza --p-trim-left-f 13 --p-trim-left-r 13 --p-trunc-len-f 150 --p-trunc-len-r 150 --o-table table.qza --o-representative-sequences rep-seqs.qza --o-denoising-stats denoising-stats.qza By inspecting the interactive demux.qzv file produced in the previous step on the Qiime2 viewer, we observe that sequence quality scores are lower than average until base #14 in both the forward and reverse reads. We will want to trim these low-quality sequences from our data, as they might interfere with downstream processing. Use the argument p-trim to specify the number of nucleotides that should be trimmed from the left end of the forward (left-f) and reverse (left-r) reads, which we define as 13 here. Similarly, the p-trunc-len argument is used to trim the right ends of, or truncate, our reads. Since we have paired-end data, our amplicons are 150 nucleotides long. We define p-trunc-len for the forward and reverse reads as 150 because we do not observe a drop off in quality scoring on their right ends in our demux.qzv file. The output rep-seqs.qza will be used in the next step of our processing workflow. 5.4 Clustering Now that we have our squeaky-clean reads, we will want to cluster them into operational taxonomic units (OTUs). We cluster our OTUs de novo. This means that we compare our sequences to each other (i.e., not against a reference database) to determine how they should be clustered. De novo OTU clustering requires more computational resources than reference-based clustering, but it has been shown to be generally more robust. An OTU is defined by some sequence homology cutoff (usually 97%). In other words, if using a 97% cutoff, reads are considered to be coming from the same OTU if at least 97% of their sequences are the same. qiime vsearch cluster-features-de-novo --i-table table.qza --i-sequences rep-seqs.qza --p-perc-identity 0.97 --o-clustered-table table-dn-97.qza --o-clustered-sequences rep-seqs-dn-97.qza 5.5 Taxonomy At this point, our OTUs lack any meaningful identification - we don‚Äôt know whether OTU ‚ÄòA‚Äô comes from the bacterium E. coli or the archaeum S. solfataricus! We determine who is present in our samples by assigning taxonomic IDs to each ‚Äúquery‚Äù sequence (from rep-seqs.qza). We do this by comparing query sequences to a database of known reference sequences ( Silva is an excellent choice for our purposes). A major advantage of using Qiime2 is that it contains the classify-sklearn algorithm, which uses machine learning via Naive Bayes to classify sequences. As is the case with other machine learning applications, the classifier must be trained. Classifier training is required for every new reference database/amplicon pair, and would be the most resource-intensive step in our workflow by far. Luckily for us, Silva is routinely used to classify sequences coming from 16S rRNA gene amplification using the 515F/806R primers, and the Qiime2 developers provide a free, pre-trained Silva classifier in their documentation for just that! Download this classifier - you will need it! At the time this was written, I used silva-132-99-515-806-nb-classifier-2018.qza, the latest version of the pre-trained Silva classifier. Even without the extra training step, it is highly recommended to run classify-sklearn on a high-performance computing cluster, as it is very memory intensive and slow (budget several hours or even a day for this to complete!). Please refer to our Quest tutorial to get started on how to submit jobs on Northwestern‚Äôs cluster if you need help getting started. qiime feature-classifier classify-sklearn --i-classifier classifier.qza --i-reads rep-seqs.qza --o-classification taxonomy.qza If you so choose, you can visualize the resultant taxonomy file on the Qiime 2 viewer to verify that classification was successful: qiime metadata tabulate --m-input-file taxonomy.qza --o-visualization taxonomy.qzv 5.6 Taxa barplots and diversity analyses in Qiime2 You can quickly visualize the community composition of your samples via the taxa barplot command. This requires your clustered feature table and taxonomy.qza from the previous step as inputs. In the Qiime 2 viewer, you can export the data that feeds the taxa barplot as .csv files specific to each level of taxonomic classification. Use those files in R to produce publication-quality figures. qiime taxa barplot --i-table table-dn-97.qza --i-taxonomy taxonomy.qza --m-metadata-file metadata.tsv --o-visualization taxa-bar-plots.qzv In fact, we don‚Äôt really want to use Qiime2 for making any sort of figure for presentations or publications, but it does have a few handy tools to quickly visualize your data to inform which types of figures you want to make. Let‚Äôs start with the built-in diversity analyses offered by Qiime2. Many diversity analyses compute diversity by incorporating phylogeny. That means we have to generate a phylogenetic tree of how our sequences are related to each other! Both rooted and unrooted trees are outputs of the align-to-tree-mafft-fasttree command. UniFrac and Faith‚Äôs Phylogenetic Diverstiy require the use of a rooted tree. qiime phylogeny align-to-tree-mafft-fasttree --i-sequences rep-seqs.qza --o-alignment aligned-rep-seqs.qza --o-masked-alignment masked-aligned-rep-seqs.qza --o-tree unrooted-tree.qza --o-rooted-tree rooted-tree.qza Additionally, the analyses we are about to perform will be subsampling our data to estimate diversity. This rarefaction is done so we can compare diversity across samples of different sizes, thereby minimizing bias. We need to know the sequencing depth we should take so we don‚Äôt miss out on too many rare sequences (by choosing too low of a depth) or too many samples themselves (by choosing too high of a depth). By making an alpha rarefaction plot that we can visualize on the Qiime2 viewer, we can make an informed decision: qiime diversity alpha-rarefaction --i-table table.qza --i-phylogeny rooted-tree.qza --p-max-depth 20000 --m-metadata-file metadata.tsv --o-visualization alpha-rarefaction.qzv From this visualization, you should choose a sequencing depth at which the observed OTUs from most samples level off, without excluding too many samples. In our example, we will choose a depth of 6500. After determining the degree of rarefaction, we can compute core diversity metrics in Qiime2: qiime diversity core-metrics-phylogenetic --i-phylogeny rooted-tree.qza --i-table table.qza --p-sampling-depth 6500 --m-metadata-file metadata.tsv --output-dir core-metrics-results The core-metrics-results folder will contain both alpha and beta diversity metrics. For each metric, you can determine diversity significance using diversity alpha-group-significance or diversity beta-group-significance: qiime diversity alpha-group-significance --i-alpha-diversity core-metrics-results/alpha-div-metric-of-interest.qza --m-metadata-file metadata.tsv --o-visualization core-metrics-results/metric-group-significance.qzv qiime diversity beta-group-significance --i-distance-matrix core-metrics-results/distance_matrix.qza --m-metadata-file metadata.tsv --m-metadata-column comparisonofinterest --o-visualization core-metrics-results/unweighted-unifrac-comparisonofinterest-significance.qzv Beta diversity visualizations can be viewed via Qiime2 View‚Äôs Emperor, which offers an interactive three-dimensional platform to explore relationships in your data. "],
["quest-tutorial.html", " 6 | Quest tutorial 6.1 Getting acquainted with Quest 6.2 Using Qiime2 on Quest 6.3 Best practices in a shared computing environment 6.4 Interactive jobs on Quest 6.5 Batch jobs on Quest 6.6 A note on partitions 6.7 More information", " 6 | Quest tutorial Created: 2019-11-19 Created by: Matt Selensky Getting an allocation on Quest You may find that you are unable to process the large volume of sequencing data on your personal computer. Thankfully, Northwestern IT offers free access to its high-performance computing cluster, Quest, to students, postdocs, and faculty. To use Quest, you first need to apply for an allocation granted by IT. Please visit this webpage to learn more about the application process. 6.1 Getting acquainted with Quest Once you obtain an allocation, you can start using Quest for any manner of processing needs. Quest is remotely accessed from your personal computer by way of a secure shell in the command line. If you use Windows, download GitBASH to be able to interact with the Unix command line in Quest. Note - this is not necessary if you use Mac or *nix. Before logging in to Quest, I recommend downloading Cyberduck, a FTP/SFTP client that makes transferring files between your personal computer and Quest much easier. See this page for instructions on how to correctly download and install Cyberduck. To log in to Quest, enter the following into the command line (or GitBASH): ssh -X &lt;netid&gt;@quest.it.northwestern.edu You will be prompted to enter your password (don‚Äôt worry, it is normal to not see the characters as you type!). In the command line, you can navigate Quest via Unix commands. For example, use cd .. to move up the file directory, then cd /projects/&lt;allocation-id&gt; to enter your project directory. Your project allocation ID will be a unique string given to you by Northwestern IT. It should be noted that your home directory (/home/&lt;net-id&gt;) is regularly backed up (up to 80 GB), but your project directory is not. 6.2 Using Qiime2 on Quest The Qiime2 software is currently available as a Docker image on the DockerHub (version 2018.8). On Quest, you can download this image via Singularity. Navigate to your project directory on Quest and run the following command: singularity pull --name qiime2-core2018-8.simg docker://qiime2/core:2018.8 This will install Qiime2 in the folder you‚Äôre currently in (which is hopefully your project directory). To use Qiime2, you will have to call the Singularity container in which it resides (/projects/&lt;allocation-id&gt;/qiime2-core2018-8.simg) every time you run a Qiime2 command. Let‚Äôs check if it correctly installed by running a help command: singularity exec /projects/&lt;allocation-id&gt;/qiime2-core2018-8.simg qiime --help If you received a bunch of ‚Äúhelp‚Äù text as an output, congratulations, Qiime2 installed correctly and is ready to be used! Before you do anything, let‚Äôs lay some ground rules first. 6.3 Best practices in a shared computing environment Quest is used by hundreds of people on campus doing Very Important Things, so following a few guidelines is in all of our best interests. First and foremost, never ever move or delete files in any folder that isn‚Äôt yours. IT will find out about it, and you will be hearing from them if you do (and rightly so). With that out of the way, feel free to store files in your home and/or project directories. Though your project directory likely has more storage, your home directory is regularly backed-up (up to 80GB). I recommend storing programming scripts or other such files in your home directory for this reason. In Quest, you shouldn‚Äôt ever run jobs on the main head node or login node. This will slow Quest‚Äôs performance for everyone. You should instead submit every script as ‚Äúinteractive‚Äù or ‚Äúbatch‚Äù jobs on designated compute nodes, following standard Slurm commands. 6.4 Interactive jobs on Quest Interactive jobs are best used for short jobs. If you submit an interactive job, your command line will be tied up for the time it takes to process your submission. If you exit the command line, your job submission will be terminated. srun --account=&lt;allocation-id&gt; --time=&lt;hh:mm:ss&gt; --partition=&lt;queue_name&gt; --mem=&lt;memory per node&gt;G --pty bash -l Append the above command to the beginning of your command. Running the same qiime --help command above in an interactive job command will look something like: module load singularity srun --account=a12345 --time=01:00:00 --partition=short --mem=18G --pty bash -l singularity exec /projects/&lt;allocation-id&gt;/qiime2-core2018-8.simg qiime --help 6.5 Batch jobs on Quest It is generally more efficient to submit scripts on Quest as batch jobs. This allows you to disconnect from Quest without prematurely stopping your submission. This is helpful if you have multi-day commands such as classifier training using sklearn in Qiime2! A batch job submission script should have the following structure (save it with a .sh file extension and upload it to Quest). To run the same help command, write the following script and save it with a .sh file extension: #!/bin/bash #SBATCH -A a12345 # Allocation #SBATCH -p short # Queue #SBATCH -t 04:00:00 # Walltime/duration of the job #SBATCH -N 1 # Number of Nodes #SBATCH --mem=18G # Memory per node in GB needed for a job. Also see --mem-per-cpu #SBATCH --ntasks-per-node=6 # Number of Cores (Processors) #SBATCH --mail-user=&lt;my_email&gt; # Designate email address for job communications #SBATCH --mail-type=&lt;event&gt; # Events options are job BEGIN, END, NONE, FAIL, REQUEUE #SBATCH --job-name=&quot;help&quot; # Name of job # unload any modules that carried over from your command line session module purge module load singularity singularity exec /projects/a12345/qiime2-core2018-8.simg qiime --help If this script is called qiime2-help.sh, simply navigate to the folder in Quest where it is stored and enter into the command line: sbatch qiime2-help.sh 6.6 A note on partitions Quest has several ‚Äúpartitions,‚Äù which are defined by how long you expect your job to take to run. Shorter jobs have shorter queues, so it would behoove you to choose the shortest partition as possible. Keep in mind, however, that your job will terminate if it runs past the time you alloted to it! Visit this webpage to learn about the different partitions and their associated maximum walltimes. 6.7 More information For more information on Quest, visit the Quest User Guide, which is excellently documented by Northwestern IT. Happy Questing! "]
]
